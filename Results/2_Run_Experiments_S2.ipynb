{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Needed imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "# LDP import\n",
    "from multi_freq_ldpy.pure_frequency_oracles.GRR import GRR_Client\n",
    "from GRR import GRR_Client\n",
    "# Fairness imports\n",
    "from GroupFairnessNotions import confusion_matrix_scorer, Statistical_parity, Metric_disparity, Equal_opportunity, Predictive_equality, Treatment_equality, Overall_accuracy, Predictive_rate_parity, CSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_seed = 20\n",
    "\n",
    "\n",
    "dataset_path = 'Datasets/Synthetic/'\n",
    "\n",
    "results_path = 'Results/'\n",
    "\n",
    "# list of epsilon\n",
    "\n",
    "\n",
    "lst_eps = [16,8,2,1,0.85,0.5,0.4,0.3,0.2,0.1]\n",
    "\n",
    "# Target attribute\n",
    "target = 'Y' \n",
    "\n",
    "# Protected attribute \n",
    "protected_att = 'A'\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "\n",
    "# Reading dataset\n",
    "dataset = 'S2' # ['S1','S3', 'S4', 'S5']\n",
    "\n",
    "# Folder where results are saved as csv files\n",
    "results_path = 'Results/'\n",
    "# Fariness metrics and info no privacy results\n",
    "header_nldp = [\"Seed_num\",\"SP_maj\", \"SP_min\", \"SPD\",\"EO_maj\",\"EO_min\",\"EOD\",\"CSP_X0_maj\", \"CSP_X0_min\", \"CSD_X0\",\"CSP_X1_maj\", \"CSP_X1_min\", \"CSD_X1\"]\n",
    "# Fariness metrics and info for privacy settings\n",
    "header_ldp = [\"Seed_num\",\"epsilon\",\"SP_maj\", \"SP_min\", \"SPD\",\"EO_maj\",\"EO_min\",\"EOD\",\"CSP_X0_maj\", \"CSP_X0_min\", \"CSD_X0\",\"CSP_X1_maj\", \"CSP_X1_min\", \"CSD_X1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to csv function for saving the results\n",
    "def write_to_csv(setting,dataset,header):\n",
    "    with open(results_path + dataset + '_'+ setting +'_results.csv', mode='a', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)    \n",
    "        writer.writerow(header)   \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>A</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X  A  Y\n",
       "0      1  0  1\n",
       "1      0  1  1\n",
       "2      0  1  1\n",
       "3      1  1  1\n",
       "4      0  1  1\n",
       "...   .. .. ..\n",
       "99995  1  0  1\n",
       "99996  0  1  1\n",
       "99997  0  1  1\n",
       "99998  0  1  1\n",
       "99999  1  1  1\n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_path + dataset + '.csv')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for no privacy setting (baseline setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "That took 26.859607934951782 seconds\n"
     ]
    }
   ],
   "source": [
    "#folder_name of the results\n",
    "setting = 'NoLDP'\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "# write head of csv file\n",
    "write_to_csv(setting,dataset,header_nldp)\n",
    "\n",
    "df_cp = copy.deepcopy(df)\n",
    "\n",
    "\n",
    "np_sp_min1, np_sp_maj1, np_sd = [], [], []\n",
    "ndp_csp_minX0, ndp_csp_majX0, np_csd_X0, ndp_csp_minX1, ndp_csp_majX1, np_csd_X1 = [], [] ,[] , [],[], []\n",
    "np_eo_min1, np_eo_maj1, np_eod = [], [], []\n",
    "\n",
    "\n",
    "for seed in range(nb_seed):\n",
    "    print(seed)\n",
    "    np.random.seed(seed) # for reproducibility\n",
    "    \n",
    "    # Use original datasets\n",
    "    X = copy.deepcopy(df_cp.drop(target, axis=1))\n",
    "    y = copy.deepcopy(df_cp[target])\n",
    "\n",
    "    # Train test splitting\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=True, stratify=y, random_state=seed)\n",
    "    \n",
    "    # instantiate and train model\n",
    "    model = RandomForestClassifier(n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # retrieving minority, majority from the test set\n",
    "    X_test_min, X_test_maj = X_test[X_test[protected_att] == 0], X_test[X_test[protected_att] == 1]\n",
    "\n",
    "    # predicted outcomes for minority, majority\n",
    "    y_pred_min, y_pred_maj = model.predict(X_test_min), model.predict(X_test_maj)\n",
    "\n",
    "    indices_min, indices_maj = X_test_min.index, X_test_maj.index\n",
    "    y_test_min, y_test_maj = y_test.get(key = indices_min), y_test.get(key = indices_maj)\n",
    "    \n",
    "    # Needed for the computation of Cond.Stat.Disp\n",
    "            \n",
    "    # retrieving four groups: A=0_X=0, A0_X=1, A1_X=0, A1_X=1 from the test set\n",
    "\n",
    "    X_test_min_X0, X_test_min_X1, X_test_maj_X0, X_test_maj_X1 = X_test[(X_test[protected_att] == 0) & (X_test['X'] == 0)], X_test[(X_test[protected_att] == 0) & (X_test['X'] == 1)], X_test[(X_test[protected_att] == 1) & (X_test['X'] == 0)], X_test[(X_test[protected_att] == 1) & (X_test['X'] == 1)]\n",
    "      \n",
    "    # confusion matrix for minority, majority\n",
    "    conf_matrix_min, conf_matrix_maj = confusion_matrix_scorer(y_test_min,y_pred_min), confusion_matrix_scorer(y_test_maj,y_pred_maj)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # predicted outcomes for the six groups\n",
    "    y_pred_A0_X0, y_pred_A0_X1,y_pred_A1_X0, y_pred_A1_X1 = model.predict(X_test_min_X0), model.predict(X_test_min_X1), model.predict(X_test_maj_X0), model.predict(X_test_maj_X1)\n",
    "\n",
    "    # computing fairness metrics\n",
    "    np_sp_min1.append(Statistical_parity(y_pred_min))\n",
    "    np_sp_maj1.append(Statistical_parity(y_pred_maj))\n",
    "    np_sd.append(Metric_disparity(Statistical_parity(y_pred_maj), Statistical_parity(y_pred_min)))\n",
    "    np_eo_min1.append(Equal_opportunity(conf_matrix_min))\n",
    "    np_eo_maj1.append(Equal_opportunity(conf_matrix_maj))\n",
    "    np_eod.append(Metric_disparity(Equal_opportunity(conf_matrix_maj), Equal_opportunity(conf_matrix_min)))    \n",
    "    ndp_csp_minX0.append(Statistical_parity(y_pred_A0_X0))\n",
    "    ndp_csp_majX0.append(Statistical_parity(y_pred_A1_X0))\n",
    "    np_csd_X0.append(Metric_disparity(Statistical_parity(y_pred_A1_X0), Statistical_parity(y_pred_A0_X0)))\n",
    "    ndp_csp_minX1.append(Statistical_parity(y_pred_A0_X1))\n",
    "    ndp_csp_majX1.append(Statistical_parity(y_pred_A1_X1))  \n",
    "    np_csd_X1.append(Metric_disparity(Statistical_parity(y_pred_A1_X1), Statistical_parity(y_pred_A0_X1)))\n",
    "    #writing the results to the csv file\n",
    "    write_to_csv(setting,dataset,[str(seed), Statistical_parity(y_pred_maj),Statistical_parity(y_pred_min),Metric_disparity(Statistical_parity(y_pred_maj), Statistical_parity(y_pred_min)),Equal_opportunity(conf_matrix_maj), Equal_opportunity(conf_matrix_min), Metric_disparity(Equal_opportunity(conf_matrix_maj), Equal_opportunity(conf_matrix_min)),Statistical_parity(y_pred_A1_X0), Statistical_parity(y_pred_A0_X0),Metric_disparity(Statistical_parity(y_pred_A1_X0), Statistical_parity(y_pred_A0_X0)), Statistical_parity(y_pred_A1_X1),Statistical_parity(y_pred_A0_X1),Metric_disparity(Statistical_parity(y_pred_A1_X1),Statistical_parity(y_pred_A0_X1))])\n",
    "\n",
    "print('That took {} seconds'.format(time.time() - starttime)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDP setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Setting 1: Local DP: only the sensitive feature is obfuscated =========\n",
      "\n",
      "16\n",
      "8\n",
      "2\n",
      "1\n",
      "0.85\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n========= Setting 1: Local DP: only the sensitive feature is obfuscated =========\\n\")\n",
    "\n",
    "#folder_name of the results\n",
    "setting = 'sLDP'\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "# write head of csv file\n",
    "write_to_csv(setting,dataset,header_ldp)\n",
    "\n",
    "df_cp = copy.deepcopy(df)\n",
    "\n",
    "\n",
    "sldp_sp_min1, sldp_sp_maj1, sldp_sd, sd_std  = [], [], [], []\n",
    "sldp_csp_minX0, sldp_csp_majX0, sldp_csd_X0, csd_X0_std, sldp_csp_minX1, sldp_csp_majX1, sldp_csd_X1, csd_X1_std= [], [] , [], [], [],[], [], []\n",
    "sldp_oa_min1, sldp_oa_maj1, sldp_oad, oad_std = [], [], [], []\n",
    "sldp_eo_min1, sldp_eo_maj1, sldp_eod, eod_std = [], [], [], []\n",
    "\n",
    "\n",
    "for epsilon in lst_eps:\n",
    "    print(epsilon)\n",
    "    \n",
    "    ldp_sp_min, ldp_sp_maj, ldp_sd = [], [], []\n",
    "    ldp_csp_minX0, ldp_csp_majX0, ldp_csd_X0, ldp_csp_minX1, ldp_csp_majX1,ldp_csd_X1 = [], [],[], [], [], []\n",
    "    ldp_oa_min, ldp_oa_maj, ldp_oad  = [], [], []\n",
    "    ldp_eo_min, ldp_eo_maj, ldp_eod  = [], [], []\n",
    "    \n",
    "    for seed in range(nb_seed):\n",
    "        #np.random.seed(seed)\n",
    "\n",
    "\n",
    "        # Preparing X and y using pandas\n",
    "        X = copy.deepcopy(df_cp.drop(target, axis=1))\n",
    "        y = copy.deepcopy(df_cp[target])\n",
    "\n",
    "        # Train test splitting\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=True, stratify=y, random_state=seed)\n",
    "\n",
    "        # Attribute's domain size\n",
    "        k = len(set(X[protected_att]))\n",
    "\n",
    "        # Applying GRR to the protected attribute of the training set\n",
    "        X_train[protected_att] = X_train[protected_att].apply(lambda x: GRR_Client(x, k, epsilon))\n",
    "        \n",
    "        # instantiate and train model\n",
    "        model = RandomForestClassifier(n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test) # prediction of the actual samples\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # retrieving minority, majority from the test set\n",
    "        X_test_min, X_test_maj = X_test[X_test[protected_att] == 0], X_test[X_test[protected_att] == 1]\n",
    "\n",
    "        # predicted outcomes for minority, majority\n",
    "        y_pred_min, y_pred_maj = model.predict(X_test_min), model.predict(X_test_maj)\n",
    "\n",
    "        indices_min, indices_maj = X_test_min.index, X_test_maj.index\n",
    "        y_test_min, y_test_maj = y_test.get(key = indices_min), y_test.get(key = indices_maj)\n",
    "        \n",
    "        \n",
    "        # Needed for the computation of Cond.Stat.Disp\n",
    "            \n",
    "        # retrieving four groups: A=0_X=0, A0_X=1, A1_X=0, A1_X=1 from the test set\n",
    "            \n",
    "        X_test_min_X0, X_test_min_X1, X_test_maj_X0, X_test_maj_X1 = X_test[(X_test[protected_att] == 0) & (X_test['X'] == 0)], X_test[(X_test[protected_att] == 0) & (X_test['X'] == 1)], X_test[(X_test[protected_att] == 1) & (X_test['X'] == 0)], X_test[(X_test[protected_att] == 1) & (X_test['X'] == 1)]\n",
    "        \n",
    "        # confusion matrix for minority, majority\n",
    "        conf_matrix_min, conf_matrix_maj = confusion_matrix_scorer(y_test_min,y_pred_min), confusion_matrix_scorer(y_test_maj,y_pred_maj)\n",
    "\n",
    "        \n",
    "        # predicted outcomes for the four groups\n",
    "        y_pred_A0_X0, y_pred_A0_X1,y_pred_A1_X0, y_pred_A1_X1 = model.predict(X_test_min_X0), model.predict(X_test_min_X1), model.predict(X_test_maj_X0), model.predict(X_test_maj_X1)\n",
    "\n",
    "        # computing fairness metrics with obfuscated A\n",
    "        ldp_sp_min.append(Statistical_parity(y_pred_min))\n",
    "        ldp_sp_maj.append(Statistical_parity(y_pred_maj))\n",
    "        ldp_sd.append(Metric_disparity(Statistical_parity(y_pred_maj), Statistical_parity(y_pred_min)))\n",
    "        \n",
    "        ldp_eo_min.append(Equal_opportunity(conf_matrix_min))   \n",
    "        ldp_eo_maj.append(Equal_opportunity(conf_matrix_maj))\n",
    "        ldp_eod.append(Metric_disparity(Equal_opportunity(conf_matrix_maj), Equal_opportunity(conf_matrix_min)))\n",
    "        \n",
    "        ldp_csp_minX0.append(Statistical_parity(y_pred_A0_X0))\n",
    "        ldp_csp_majX0.append(Statistical_parity(y_pred_A1_X0))\n",
    "        ldp_csd_X0.append(Metric_disparity(Statistical_parity(y_pred_A1_X0), Statistical_parity(y_pred_A0_X0)))\n",
    "        \n",
    "        ldp_csp_minX1.append(Statistical_parity(y_pred_A0_X1))\n",
    "        ldp_csp_majX1.append(Statistical_parity(y_pred_A1_X1))\n",
    "        ldp_csd_X1.append(Metric_disparity(Statistical_parity(y_pred_A1_X1), Statistical_parity(y_pred_A0_X1)))\n",
    "        write_to_csv(setting,dataset,[str(seed), str(epsilon), Statistical_parity(y_pred_maj),Statistical_parity(y_pred_min),Metric_disparity(Statistical_parity(y_pred_maj), Statistical_parity(y_pred_min)),Equal_opportunity(conf_matrix_maj), Equal_opportunity(conf_matrix_min), Metric_disparity(Equal_opportunity(conf_matrix_maj), Equal_opportunity(conf_matrix_min)),Statistical_parity(y_pred_A1_X0), Statistical_parity(y_pred_A0_X0),Metric_disparity(Statistical_parity(y_pred_A1_X0), Statistical_parity(y_pred_A0_X0)), Statistical_parity(y_pred_A1_X1),Statistical_parity(y_pred_A0_X1),Metric_disparity(Statistical_parity(y_pred_A1_X1),Statistical_parity(y_pred_A0_X1))])\n",
    "    sldp_sp_min1.append(np.mean(ldp_sp_min))\n",
    "    sldp_sp_maj1.append(np.mean(ldp_sp_maj))\n",
    "    sldp_sd.append(np.mean(ldp_sd))\n",
    "    sd_std.append(np.std(sldp_sd))\n",
    "    sldp_csp_minX0.append(np.mean(ldp_csp_minX0))\n",
    "    sldp_csp_majX0.append(np.mean(ldp_csp_majX0))\n",
    "    sldp_csd_X0.append(np.mean(ldp_csd_X0))\n",
    "    csd_X0_std.append(np.std(sldp_csd_X0))\n",
    "    sldp_csp_minX1.append(np.mean(ldp_csp_minX1))\n",
    "    sldp_csp_majX1.append(np.mean(ldp_csp_majX1))\n",
    "    sldp_csd_X1.append(np.mean(ldp_csd_X1))\n",
    "    csd_X1_std.append(np.std(sldp_csd_X1))\n",
    "    sldp_eo_min1.append(np.mean(ldp_eo_min))\n",
    "    sldp_eo_maj1.append(np.mean(ldp_eo_maj))\n",
    "    sldp_eod.append(np.mean(ldp_eod))\n",
    "    eod_std.append(np.std(sldp_eod))\n",
    "    #writing the results to the csv file\n",
    "    #write_to_csv(setting,dataset,[str(seed), str(epsilon), Statistical_parity(y_pred_maj),Statistical_parity(y_pred_min),Metric_disparity(Statistical_parity(y_pred_maj), Statistical_parity(y_pred_min)),Equal_opportunity(conf_matrix_maj), Equal_opportunity(conf_matrix_min), Metric_disparity(Equal_opportunity(conf_matrix_maj), Equal_opportunity(conf_matrix_min)),Statistical_parity(y_pred_A1_X0), Statistical_parity(y_pred_A0_X0),Metric_disparity(Statistical_parity(y_pred_A1_X0), Statistical_parity(y_pred_A0_X0)), Statistical_parity(y_pred_A1_X1),Statistical_parity(y_pred_A0_X1),Metric_disparity(Statistical_parity(y_pred_A1_X1),Statistical_parity(y_pred_A0_X1))])\n",
    "print('That took {} seconds'.format(time.time() - starttime)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
